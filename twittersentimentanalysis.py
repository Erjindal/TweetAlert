# -*- coding: utf-8 -*-
"""TwitterSentimentAnalysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XugWBbzpyp2E8NR0KNAX7wBRb4TFNJKV
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, cross_val_score

from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline
from sklearn.metrics import f1_score

import time
from time import time

tweets = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

print("Train shape: ", tweets.shape)
print("Test shape ", test_data.shape)

tweets.head()

tweets.info()

tweets.isnull().sum()

# Creating a table to track results
results = pd.DataFrame(columns=[
                    "Experiment Name",
                    "Cross fold train F1",
                    "Val F1",
                    "Val Acc",
                    "Train Taken (s)",
                    "Test Time (s)"
            ])

tfidf = TfidfVectorizer()

X_train = tfidf.fit_transform(tweets["text"])
y_train = tweets["target"]
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.25, random_state=42)

lr = LogisticRegression()

start = time()
lr.fit(X_train, y_train)
train_time = round(time() - start, 3)

start = time()
y_pred = lr.predict(X_test)
test_time = round(time() - start, 3)

pred_prob = lr.predict_proba(X_test)
accuracy = lr.score(X_test, y_test)
f1 = f1_score(y_test, y_pred)

print('Accuracy of Logistic Regression on test set: {:.2f}'.format(accuracy))
print('F1 score of Logisitic Regression on test set: {:.3f}'.format(f1))

results.loc[0] = ["Baseline LR",
                  "-",
                  round(f1, 3),
                  round(lr.score(X_test, y_test), 3),
                  train_time,
                  test_time
                 ]

results

# Preparing data for grid search
X_train = tweets['text']
y_train = tweets['target']

X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, stratify=y_train, test_size=0.25, random_state=42)

lr_pipe = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', LogisticRegression())
])

parameters = {
    'tfidf__use_idf': [True, False],
    'tfidf__norm': ['l1', 'l2'],
    'clf__solver': ['liblinear', 'saga'],
    'clf__penalty': ('l1', 'l2'),
    # 'clf__tol': (0.0001, 0.00001, 0.0000001),
    'clf__C': (10, 1, 0.1, 0.01),
    'clf__max_iter': (50, 80, 200),
}

# K fold
strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)

lr_gs = GridSearchCV(lr_pipe, parameters, scoring = 'f1', cv = strat_kf, n_jobs=-1, verbose = 2)
lr_gs.fit(X_train, y_train)

print("F1 score for the model with best parameters:", round(lr_gs.best_score_, 3))
print("Best parameters: ", lr_gs.best_params_)

# Best estimator fitting time
start = time()
lr_gs.best_estimator_.fit(X_train, y_train)
train_time = round(time() - start, 4)

start = time()
y_pred = lr_gs.best_estimator_.predict(X_test)
test_time = round(time() - start, 3)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy Score on the test set: ", round(accuracy, 3))
print("F1 Score on the test set: ", round(f1, 3))

# Building a confusion matric on the test data
conf_mat = confusion_matrix(y_test, y_pred)

plt.figure(figsize = (5,3))

ax = sns.heatmap(conf_mat, annot=True, fmt=".0f")

# set x-axis label and ticks.
ax.set_xlabel("Predicted labels", fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Non Toxic', 'Toxic'])

# set y-axis label and ticks
ax.set_ylabel("Actual Labels", fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Non Toxic', 'Toxic'])

ax.set_title("Confusion Matrix on Test Data", fontsize=14, pad=20)

plt.show()

print(classification_report(y_test, y_pred))

results.loc[len(results)] = [
        "Logistic Regression",
        round(lr_gs.best_score_, 3),
        round(f1, 3),
        round(accuracy, 3),
        train_time,
        test_time
]

results

sgd_pipe = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', SGDClassifier())
])

parameters = {
    'tfidf__use_idf': [True, False],
    'tfidf__norm': ['l1', 'l2'],
    'clf__loss': ('hinge', 'perceptron', 'log_loss'),
    'clf__penalty': ('l1', 'l2', 'elasticnet'),
    'clf__tol': (0.0001, 0.00001, 0.0000001),
    'clf__alpha': (0.1, 0.01, 0.001, 0.0001),
}


# K fold
strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)

sgd_gs = GridSearchCV(sgd_pipe, parameters, scoring = 'f1', cv = strat_kf, n_jobs=-1, verbose = 2)
sgd_gs.fit(X_train, y_train)

print("F1 score for the model with best parameters:", round(sgd_gs.best_score_, 3))
print("Best parameters: ", sgd_gs.best_params_)

# Best estimator fitting time
start = time()
sgd_gs.best_estimator_.fit(X_train, y_train)
train_time = round(time() - start, 4)

start = time()
y_pred = sgd_gs.best_estimator_.predict(X_test)
test_time = round(time() - start, 3)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy Score on the test set: ", round(accuracy, 3))
print("F1 Score on the test set: ", round(f1, 3))

# Building a confusion matric on the test data
conf_mat = confusion_matrix(y_test, y_pred)

plt.figure(figsize = (5,3))

ax = sns.heatmap(conf_mat, annot=True, fmt=".0f")

# set x-axis label and ticks.
ax.set_xlabel("Predicted labels", fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Non Toxic', 'Toxic'])

# set y-axis label and ticks
ax.set_ylabel("Actual Labels", fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Non Toxic', 'Toxic'])

ax.set_title("Confusion Matrix on Test Data", fontsize=14, pad=20)

plt.show()

print(classification_report(y_test, y_pred))

results.loc[len(results)] = [
        "Stochastic GD Classifier",
        round(sgd_gs.best_score_, 3),
        round(f1, 3),
        round(accuracy, 3),
        train_time,
        test_time
]

results

rfc_pipe = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', RandomForestClassifier())
])

parameters = {
    'tfidf__use_idf': [True, False],
    'tfidf__norm': ['l1', 'l2'],
    'clf__max_depth': [None, 9, 15, 22],
    'clf__min_samples_split': [2, 5, 10],
    'clf__min_samples_leaf': [2, 5, 10],
    'clf__n_estimators':[80, 150, 200],
    'clf__criterion':['gini','entropy']
}

# K fold
strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)

rfc_gs = GridSearchCV(rfc_pipe, parameters, scoring = 'f1', cv = strat_kf, n_jobs=-1, verbose = 2)
rfc_gs.fit(X_train, y_train)

print("Score for the model with best parameters:", round(rfc_gs.best_score_, 3))
print("Best parameters: ", rfc_gs.best_params_)

# Best estimator fitting time
start = time()
rfc_gs.best_estimator_.fit(X_train, y_train)
train_time = round(time() - start, 4)

start = time()
y_pred = rfc_gs.best_estimator_.predict(X_test)
test_time = round(time() - start, 3)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy Score on the test set: ", round(accuracy, 3))
print("F1 Score on the test set: ", round(f1, 3))

# Building a confusion matric on the test data
conf_mat = confusion_matrix(y_test, y_pred)

plt.figure(figsize = (5,3))

ax = sns.heatmap(conf_mat, annot=True, fmt=".0f")

# set x-axis label and ticks.
ax.set_xlabel("Predicted labels", fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Non Toxic', 'Toxic'])

# set y-axis label and ticks
ax.set_ylabel("Actual Labels", fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Non Toxic', 'Toxic'])

ax.set_title("Confusion Matrix on Test Data", fontsize=14, pad=20)

plt.show()

print(classification_report(y_test, y_pred))

results.loc[len(results)] = [
        "Random Forest Classifier",
        round(rfc_gs.best_score_, 3),
        round(f1, 3),
        round(accuracy, 3),
        train_time,
        test_time
]

results

svc_pipe = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', SVC())
])

parameters = {
    'tfidf__use_idf': [True, False],
    'tfidf__norm': ['l1', 'l2'],
    'clf__kernel': ('rbf', 'poly'),
    'clf__degree': (1, 2, 3, 4, 5),
    'clf__C': (10, 1, 0.1, 0.01),
}


# K fold
strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)

svc_gs = GridSearchCV(svc_pipe, parameters, scoring = 'f1', cv = strat_kf, n_jobs=-1, verbose = 2)
svc_gs.fit(X_train, y_train)

print("Score for the model with best parameters:", round(svc_gs.best_score_, 3))
print("Best parameters: ", svc_gs.best_params_)

# Best estimator fitting time
start = time()
svc_gs.best_estimator_.fit(X_train, y_train)
train_time = round(time() - start, 4)

start = time()
y_pred = svc_gs.best_estimator_.predict(X_test)
test_time = round(time() - start, 3)
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy Score on the test set: ", round(accuracy, 3))
print("F1 Score on the test set: ", round(f1, 3))

# Building a confusion matric on the test data
conf_mat = confusion_matrix(y_test, y_pred)

plt.figure(figsize = (5,3))

ax = sns.heatmap(conf_mat, annot=True, fmt=".0f")

# set x-axis label and ticks.
ax.set_xlabel("Predicted labels", fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Non Toxic', 'Toxic'])

# set y-axis label and ticks
ax.set_ylabel("Actual Labels", fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Non Toxic', 'Toxic'])

ax.set_title("Confusion Matrix on Test Data", fontsize=14, pad=20)

plt.show()

print(classification_report(y_test, y_pred))

results.loc[len(results)] = [
        "Support Vector Classifier",
        round(svc_gs.best_score_, 3),
        round(f1, 3),
        round(accuracy, 3),
        train_time,
        test_time
]

results

lr_vc = lr_gs.best_estimator_
sgd_vc = sgd_gs.best_estimator_
rfc_vc = rfc_gs.best_estimator_
svc_vc = svc_gs.best_estimator_

estimators = [
    ('lr', lr_vc),
    ('sgd', sgd_vc),
    ('rfc', rfc_vc),
    ('svc', sgd_vc)
]

strat_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=15)

vc_clf = VotingClassifier(estimators=estimators, voting='hard')
scores = cross_val_score(vc_clf, X_train, y_train, scoring='f1', cv=strat_kf)

start = time()
print('F1 Score on training set: ', round(scores.mean(), 3))
ensemble_model = vc_clf.fit(X_train, y_train)
train_time = round(time() - start, 3)

start = time()
y_pred = ensemble_model.predict(X_test)
test_time = round(time() - start, 3)

accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print("Accuracy Score on the test set: ", round(accuracy, 3))
print("F1 Score on the test set: ", round(f1, 3))

# Building a confusion matric on the test data
conf_mat = confusion_matrix(y_test, y_pred)

plt.figure(figsize = (5,3))

ax = sns.heatmap(conf_mat, annot=True, fmt=".0f")

# set x-axis label and ticks.
ax.set_xlabel("Predicted labels", fontsize=14, labelpad=20)
ax.xaxis.set_ticklabels(['Non Toxic', 'Toxic'])

# set y-axis label and ticks
ax.set_ylabel("Actual Labels", fontsize=14, labelpad=20)
ax.yaxis.set_ticklabels(['Non Toxic', 'Toxic'])

ax.set_title("Confusion Matrix on Test Data", fontsize=14, pad=20)

plt.show()

print(classification_report(y_test, y_pred))

results.loc[len(results)] = [
        "Voting Classifier",
        round(scores.mean(), 3),
        round(f1, 3),
        round(accuracy, 3),
        train_time,
        test_time
]

results

# Retraining with full dataset
tweets = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

X_train, y_train = tweets['text'], tweets['target']
X_test = test_data['text']

svc_gs.best_estimator_.fit(X_train, y_train)
y_pred = svc_gs.best_estimator_.predict(X_test)
test_data['target'] = y_pred
test_data.drop(['text','keyword','location'], axis=1, inplace=True)

test_data.head()